n-gram
    train()
        read in file (test file)
        pre-process text -> list of sentences (token form) (helper)
        calculate n-gram possibilities
            (generalize into helper method? Possibly customized to allow for uni/bi/tri selection?)
        format output (n-gram + log_2(probability))

    score()
        read in file
        pre-process text (helper)
        calculate sentence probabilities
            (generalize into helper method? Possibly customized to allow for uni/bi/tri selection?)
        calculate perplexity (helper likely)
        format output

helpers

    Pre_process() (partially started in Jessie's code)
        (taken from instruction file as base)
        # step 1: read in the data from the file as a chunk of text
        # step 2: convert the text into a list of sentences, stripping (I am working \n Hello there!) -> [I am working, Hello there!]
        #newline characters (\n) (I am working \n Hello there!) -> (I am working
                                                                    Hello there!)
        # step 3: represent each sentence as a list of tokens [I am working, Hello there!] -> [[I, am, working], [Hello, there!]]
        # step 4: insert start and stop tokens [[I, am, working], [Hello, there!]] -> [[<s>, I, am, working, </s>], [<s>, Hello, there!, </s>]]
        # step 5: return the list of lists

    n-gram_prob()
        (incorporate laplace smoothing. <UNK>ing here + sentence_prob? or in pre-process with train() + score() selection.


